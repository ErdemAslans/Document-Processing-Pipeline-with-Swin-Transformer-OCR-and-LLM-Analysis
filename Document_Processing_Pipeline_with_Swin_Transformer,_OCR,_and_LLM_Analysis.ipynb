{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP2MzEJS9/+d5daRpnQwOtx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErdemAslans/Document-Processing-Pipeline-with-Swin-Transformer-OCR-and-LLM-Analysis/blob/main/Document_Processing_Pipeline_with_Swin_Transformer%2C_OCR%2C_and_LLM_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwGRv2nFWea7",
        "outputId": "a39ec0fc-882b-4e6a-e883-eb3ae3876bfe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.31.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.31.2-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading unstructured_client-0.31.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading unstructured_client-0.31.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading unstructured_client-0.30.6-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading unstructured_client-0.30.5-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading unstructured_client-0.30.4-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading unstructured_client-0.30.3-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is still looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading unstructured_client-0.30.2-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading unstructured_client-0.30.1-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading unstructured_client-0.30.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading unstructured_client-0.29.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading unstructured_client-0.28.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading unstructured_client-0.27.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Collecting eval-type-backport<0.3.0,>=0.2.0 (from unstructured-client->unstructured)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.27.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=83c61a3d48d775557c895a63df947ccd6a59f5f61baf5d255a2d0a7a7f868af9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pydub, filetype, uvicorn, tomlkit, semantic-version, ruff, rapidfuzz, python-multipart, python-magic, python-iso639, pypdf, pydantic-core, olefile, mypy-extensions, marshmallow, langdetect, jsonpath-python, groovy, ffmpy, eval-type-backport, emoji, backoff, aiofiles, typing-inspect, starlette, python-oxmsg, pydantic, unstructured-client, safehttpx, gradio-client, fastapi, dataclasses-json, unstructured, gradio\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "Successfully installed aiofiles-23.2.1 backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.14.1 eval-type-backport-0.2.2 fastapi-0.115.12 ffmpy-0.5.0 filetype-1.2.0 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.0.0 olefile-0.47 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 pypdf-5.4.0 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 rapidfuzz-3.12.2 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 typing-inspect-0.9.0 unstructured-0.17.2 unstructured-client-0.27.0 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pymongo"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyA0emEeNOpM",
        "outputId": "2738e465-d4fa-4975-995d-882680898378"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'unstructured[image]'\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MvlX6dgzKFck",
        "outputId": "559afb8b-6b5c-445f-acb8-40a2b2092fc5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[image] in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (5.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (4.13.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (2.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (3.12.2)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (0.27.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (1.1)\n",
            "Collecting onnx>=1.17.0 (from unstructured[image])\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime>=1.19.0 (from unstructured[image])\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting pdf2image (from unstructured[image])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pdfminer.six (from unstructured[image])\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pikepdf (from unstructured[image])\n",
            "  Downloading pikepdf-9.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pi-heif (from unstructured[image])\n",
            "  Downloading pi_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[image]) (5.4.0)\n",
            "Collecting google-cloud-vision (from unstructured[image])\n",
            "  Downloading google_cloud_vision-3.10.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting effdet (from unstructured[image])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting unstructured-inference>=0.8.10 (from unstructured[image])\n",
            "  Downloading unstructured_inference-0.8.10-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[image])\n",
            "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.17.0->unstructured[image]) (5.29.3)\n",
            "Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[image])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[image]) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[image]) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[image]) (1.13.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (0.29.3)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (2.6.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (1.0.15)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (4.49.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[image]) (1.14.1)\n",
            "Collecting pypdfium2 (from unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[image]) (11.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured[image]) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[image]) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[image]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[image]) (0.21.0+cu124)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[image]) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[image])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[image]) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[image]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[image]) (1.26.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[image]) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[image]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[image]) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[image]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[image]) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[image]) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[image]) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pikepdf->unstructured[image]) (1.2.18)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured[image]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[image]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[image]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[image]) (2025.1.31)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (0.28.1)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (1.0.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (1.6.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (2.9.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[image]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[image]) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[image]) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[image]) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[image]) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[image]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[image]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[image]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[image]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[image]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[image]) (0.14.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[image])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[image]) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[image]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[image]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[image]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[image]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[image]) (3.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[image]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[image]) (2.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->unstructured-inference>=0.8.10->unstructured[image]) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unstructured-inference>=0.8.10->unstructured[image])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[image]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[image]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[image]) (0.21.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[image]) (1.0.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[image])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=0.8.10->unstructured[image]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=0.8.10->unstructured[image]) (2025.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[image]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[image]) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[image]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured-inference>=0.8.10->unstructured[image]) (3.0.2)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.8.10-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.10.1-py3-none-any.whl (526 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.1/526.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pikepdf-9.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=22175eaf48add467fac7ea0d4299d8a01bf9f7e6152c7a9cf1e6fbcd0900d711\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, unstructured.pytesseract, pypdfium2, pi-heif, pdf2image, onnx, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, pikepdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, google-cloud-vision, unstructured-inference, effdet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 effdet-0.4.1 google-cloud-vision-3.10.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.21.0 pdf2image-1.17.0 pdfminer.six-20240706 pi-heif-0.22.0 pikepdf-9.5.2 pypdfium2-4.30.1 unstructured-inference-0.8.10 unstructured.pytesseract-0.3.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "a52aa7c2e5bb4a5cbab04b4ca4512459"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import SwinForImageClassification, AutoProcessor, pipeline\n",
        "from unstructured.partition.auto import partition\n",
        "from unstructured.staging.base import elements_to_text\n",
        "import requests\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import time\n",
        "import json\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import shutil\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive bağlandı.\")\n",
        "\n",
        "CLASSES = [\n",
        "    'letter', 'form', 'email', 'handwritten', 'advertisement',\n",
        "    'scientific_report', 'scientific_publication', 'specification',\n",
        "    'file_folder', 'news_article', 'budget', 'invoice',\n",
        "    'presentation', 'questionnaire', 'resume', 'memo'\n",
        "]\n"
      ],
      "metadata": {
        "id": "1dsLzzcf8qap"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(data_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Veri setini train/val/test alt klasörlerine böler.\n",
        "\n",
        "    Args:\n",
        "        data_dir: RVL_CDIP_small veri setinin yolu\n",
        "        train_ratio: Eğitim için kullanılacak veri oranı\n",
        "        val_ratio: Doğrulama için kullanılacak veri oranı\n",
        "        test_ratio: Test için kullanılacak veri oranı\n",
        "        seed: Rastgelelik için tohum değeri\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    train_dir = os.path.join(data_dir, 'train')\n",
        "    val_dir = os.path.join(data_dir, 'val')\n",
        "    test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "    for directory in [train_dir, val_dir, test_dir]:\n",
        "        if os.path.exists(directory):\n",
        "            shutil.rmtree(directory)\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    for class_name in CLASSES:\n",
        "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "    # Sınıf başına dosya sayılarını tutacak sayaçlar\n",
        "    class_counts = {cls: {\"train\": 0, \"val\": 0, \"test\": 0} for cls in CLASSES}\n",
        "\n",
        "    for class_name in tqdm(CLASSES, desc=\"Sınıflar hazırlanıyor\"):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            print(f\"Uyarı: {class_name} sınıfı için klasör bulunamadı.\")\n",
        "            continue\n",
        "\n",
        "        files = [f for f in os.listdir(class_dir) if f.endswith(('.tif', '.jpg', '.png', '.jpeg', '.pdf'))]\n",
        "\n",
        "        # Sınıfta çok az dosya varsa bile veri kümesi dengeli olsun\n",
        "        if len(files) < 10:\n",
        "            print(f\"Uyarı: {class_name} sınıfında sadece {len(files)} örnek var. Veri artırma düşünülebilir.\")\n",
        "\n",
        "        random.shuffle(files)\n",
        "\n",
        "        n_files = len(files)\n",
        "        n_train = int(n_files * train_ratio)\n",
        "        n_val = int(n_files * val_ratio)\n",
        "\n",
        "        for i, file in enumerate(files):\n",
        "            src = os.path.join(class_dir, file)\n",
        "            if i < n_train:\n",
        "                dst = os.path.join(train_dir, class_name, file)\n",
        "                class_counts[class_name][\"train\"] += 1\n",
        "            elif i < n_train + n_val:\n",
        "                dst = os.path.join(val_dir, class_name, file)\n",
        "                class_counts[class_name][\"val\"] += 1\n",
        "            else:\n",
        "                dst = os.path.join(test_dir, class_name, file)\n",
        "                class_counts[class_name][\"test\"] += 1\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "    # Özet tablo oluştur\n",
        "    summary_table = []\n",
        "    for cls in CLASSES:\n",
        "        summary_table.append([\n",
        "            cls,\n",
        "            class_counts[cls][\"train\"],\n",
        "            class_counts[cls][\"val\"],\n",
        "            class_counts[cls][\"test\"],\n",
        "            sum(class_counts[cls].values())\n",
        "        ])\n",
        "\n",
        "    summary_df = pd.DataFrame(\n",
        "        summary_table,\n",
        "        columns=[\"Sınıf\", \"Eğitim\", \"Doğrulama\", \"Test\", \"Toplam\"]\n",
        "    )\n",
        "\n",
        "    print(\"\\nVeri Seti Dağılımı:\")\n",
        "    print(summary_df)\n",
        "    print(\"\\nToplam:\")\n",
        "    print(f\"Eğitim: {summary_df['Eğitim'].sum()} örnek\")\n",
        "    print(f\"Doğrulama: {summary_df['Doğrulama'].sum()} örnek\")\n",
        "    print(f\"Test: {summary_df['Test'].sum()} örnek\")\n",
        "    print(f\"Genel Toplam: {summary_df['Toplam'].sum()} örnek\")\n",
        "\n",
        "    print(\"\\nVeri seti hazırlama tamamlandı.\")\n",
        "    print(f\"Eğitim: {train_dir}\")\n",
        "    print(f\"Doğrulama: {val_dir}\")\n",
        "    print(f\"Test: {test_dir}\")\n",
        "\n",
        "    return train_dir, val_dir, test_dir\n"
      ],
      "metadata": {
        "id": "ZDTapdU68s1N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentDataset(Dataset):\n",
        "    def __init__(self, root_dir, processor, class_to_idx=None, transform=None, is_training=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.is_training = is_training\n",
        "\n",
        "        # İstatistikler için sayaçlar\n",
        "        self.tif_count = 0\n",
        "        self.other_count = 0\n",
        "        self.error_count = 0\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(CLASSES)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        # Sınıfları ve örnekleri yükle\n",
        "        valid_classes = []\n",
        "        for class_name in os.listdir(root_dir):\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            if os.path.isdir(class_dir) and class_name in self.class_to_idx:\n",
        "                valid_classes.append(class_name)\n",
        "                for filename in os.listdir(class_dir):\n",
        "                    # Büyük/küçük harf duyarsız kontrol\n",
        "                    lower_filename = filename.lower()\n",
        "                    if lower_filename.endswith(('.tif', '.tiff')):\n",
        "                        self.tif_count += 1\n",
        "                        image_path = os.path.join(class_dir, filename)\n",
        "                        self.samples.append((image_path, self.class_to_idx.get(class_name, 0)))\n",
        "                    elif lower_filename.endswith(('.jpg', '.jpeg', '.png', '.pdf')):\n",
        "                        self.other_count += 1\n",
        "                        image_path = os.path.join(class_dir, filename)\n",
        "                        self.samples.append((image_path, self.class_to_idx.get(class_name, 0)))\n",
        "\n",
        "        print(f\"Veri seti yüklendi: {len(self.samples)} belge, {len(valid_classes)} sınıf\")\n",
        "        print(f\"TIF dosyaları: {self.tif_count}, Diğer dosyalar: {self.other_count}\")\n",
        "\n",
        "        if len(valid_classes) < len(CLASSES):\n",
        "            missing = set(CLASSES) - set(valid_classes)\n",
        "            print(f\"Uyarı: {missing} sınıfları bulunamadı.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Görüntüyü yükle\n",
        "            image = self._load_image(image_path)\n",
        "\n",
        "            # ÖNEMLİ DÜZELTME: Ya transform kullan ya da processor, ikisini birlikte değil\n",
        "            if self.is_training and self.transform:\n",
        "                # Eğitim sırasında veri artırma için transform kullan\n",
        "                pixel_values = self.transform(image)\n",
        "            else:\n",
        "                # Normal işleme için processor kullan\n",
        "                # Processor içinde zaten dönüşüm var, o yüzden processor'ı direkt PIL görüntüsüne uygula\n",
        "                inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "                pixel_values = inputs[\"pixel_values\"].squeeze()\n",
        "\n",
        "            return {\n",
        "                'pixel_values': pixel_values,\n",
        "                'labels': torch.tensor(label, dtype=torch.long),\n",
        "                'path': image_path\n",
        "            }\n",
        "        except Exception as e:\n",
        "            self.error_count += 1\n",
        "            if self.error_count <= 5:  # Sadece ilk birkaç hatayı göster\n",
        "                print(f\"Görüntü işleme hatası ({image_path}): {e}\")\n",
        "\n",
        "            # Hata durumunda varsayılan tensör döndür\n",
        "            return {\n",
        "                'pixel_values': torch.zeros((3, 224, 224)),\n",
        "                'labels': torch.tensor(label, dtype=torch.long),\n",
        "                'path': image_path\n",
        "            }\n",
        "\n",
        "    def _load_image(self, image_path):\n",
        "        \"\"\"TIF formatını destekleyen görüntü yükleme fonksiyonu\"\"\"\n",
        "        try:\n",
        "            # PIL ile görüntüyü açma\n",
        "            with Image.open(image_path) as img:\n",
        "                return img.convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"PIL ile yükleme hatası ({image_path}): {e}\")\n",
        "\n",
        "            # TIF dosyaları için alternatif yöntem\n",
        "            try:\n",
        "                import imageio.v2 as imageio  # v2 kullan (deprecation uyarısını önlemek için)\n",
        "                img_array = imageio.imread(image_path)\n",
        "\n",
        "                # 2D (gri tonlamalı) bir görüntüyse 3 kanala dönüştür\n",
        "                if len(img_array.shape) == 2:\n",
        "                    img_array = np.stack([img_array, img_array, img_array], axis=2)\n",
        "                elif len(img_array.shape) == 3 and img_array.shape[2] > 3:\n",
        "                    # RGBA veya başka çok kanallı formatta ise, ilk 3 kanalı al\n",
        "                    img_array = img_array[:, :, :3]\n",
        "\n",
        "                # NumPy dizisini PIL görüntüsüne dönüştür\n",
        "                return Image.fromarray(np.uint8(img_array))\n",
        "            except Exception as e2:\n",
        "                print(f\"Alternatif yükleme hatası ({image_path}): {e2}\")\n",
        "                # En kötü durumda gri bir görüntü döndür\n",
        "                return Image.new('RGB', (224, 224), color='gray')"
      ],
      "metadata": {
        "id": "83jKCRiQ8xYf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinImageProcessor:\n",
        "    def __init__(self, image_size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "        self.image_size = image_size\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        # Temel dönüşüm\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "        # Eğitim için veri artırma dönüşümleri\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.Resize((int(image_size*1.1), int(image_size*1.1))),\n",
        "            transforms.RandomCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, images, return_tensors=\"pt\"):\n",
        "        \"\"\"\n",
        "        Görüntüleri işleyip model için hazırlar - TIF formatları için optimize edilmiş\n",
        "        \"\"\"\n",
        "        if not isinstance(images, list):\n",
        "            images = [images]\n",
        "\n",
        "        pixel_values = []\n",
        "        for image in images:\n",
        "            try:\n",
        "                # PIL.Image kontrolünü kaldırıldı, herhangi bir formattaki görüntüyü işlemeye çalışacak\n",
        "                pixel_value = self.transform(image)\n",
        "                pixel_values.append(pixel_value)\n",
        "            except Exception as e:\n",
        "                print(f\"Görüntü dönüşüm hatası: {e}\")\n",
        "                # Hata durumunda varsayılan bir tensör oluştur\n",
        "                pixel_values.append(torch.zeros((3, self.image_size, self.image_size)))\n",
        "\n",
        "        # Tensörleri yığınla\n",
        "        if return_tensors == \"pt\":\n",
        "            pixel_values = torch.stack(pixel_values)\n",
        "\n",
        "        return {\"pixel_values\": pixel_values}\n",
        "\n",
        "    def get_train_transform(self):\n",
        "        \"\"\"Eğitim için veri artırma dönüşümlerini döndürür\"\"\"\n",
        "        return self.train_transform"
      ],
      "metadata": {
        "id": "OkDotsue88EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentClassifier:\n",
        "    def __init__(self, num_classes=16, pretrained_model=\"microsoft/swin-base-patch4-window7-224-in22k\", model_path='/content/drive/MyDrive/RVL_CDIP_small/best_document_classifier.pth'):\n",
        "        from transformers import SwinForImageClassification\n",
        "        self.processor = SwinImageProcessor(image_size=224)\n",
        "\n",
        "        self.device = torch.device(\"cpu\")\n",
        "\n",
        "        # Önce kontrol et: Eğitilmiş model var mı?\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            print(f\"Eğitilmiş model bulundu, yükleniyor: {model_path}\")\n",
        "\n",
        "            # Doğrudan pretrained modeli çağırıp sonra state_dict değiştirmek yerine\n",
        "            # Modelin yapısına göre state_dict'i yükle\n",
        "\n",
        "            # Önce model state_dict'ini yükle\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "            # Modeli oluştur - API değişikliklerinden etkilenmemek için from_pretrained kullan\n",
        "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "                # Checkpoint bir dictionary ise ve içinde model_state_dict varsa\n",
        "\n",
        "                # Geçici bir model oluştur ve yapısını kopyala\n",
        "                print(\"Geçici model yapısı oluşturuluyor...\")\n",
        "                temp_model = SwinForImageClassification.from_pretrained(\n",
        "                    pretrained_model,\n",
        "                    num_labels=num_classes,\n",
        "                    ignore_mismatched_sizes=True\n",
        "                )\n",
        "\n",
        "                # Yeni bir model oluştur ve sadece yapılandırmasını kullan\n",
        "                self.model = SwinForImageClassification(temp_model.config)\n",
        "\n",
        "                # Eğitilmiş model ağırlıklarını yükle\n",
        "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(\"Model ağırlıkları başarıyla yüklendi!\")\n",
        "\n",
        "                # Geçici modeli bellekten temizle\n",
        "                import gc\n",
        "                del temp_model\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "            else:\n",
        "\n",
        "                print(\"Geçici model yapısı oluşturuluyor...\")\n",
        "                temp_model = SwinForImageClassification.from_pretrained(\n",
        "                    pretrained_model,\n",
        "                    num_labels=num_classes,\n",
        "                    ignore_mismatched_sizes=True\n",
        "                )\n",
        "\n",
        "                # Yeni model oluştur\n",
        "                self.model = SwinForImageClassification(temp_model.config)\n",
        "\n",
        "                # Ağırlıkları yükle\n",
        "                self.model.load_state_dict(checkpoint)\n",
        "                print(\"Model ağırlıkları başarıyla yüklendi!\")\n",
        "\n",
        "                # Geçici modeli temizle\n",
        "                import gc\n",
        "                del temp_model\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "        else:\n",
        "            # Eğitilmiş model yoksa pretrained modeli indir\n",
        "            print(f\"Eğitilmiş model bulunamadı. Pretrained model indiriliyor.\")\n",
        "            self.model = SwinForImageClassification.from_pretrained(\n",
        "                pretrained_model,\n",
        "                num_labels=num_classes,\n",
        "                ignore_mismatched_sizes=True\n",
        "            )\n",
        "\n",
        "        print(f\"Model {self.device} üzerinde çalışacak\")\n",
        "        self.model.to(self.device)\n",
        "        self.idx_to_class = {i: cls for i, cls in enumerate(CLASSES)}\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=10, learning_rate=2e-5, weight_decay=0.01, progress=None):\n",
        "        \"\"\"\n",
        "        İyileştirilmiş eğitim fonksiyonu:\n",
        "        - Weight decay eklenmiş AdamW optimizer\n",
        "        - Cosine annealing öğrenme oranı çizelgesi\n",
        "        - Karışıklık matrisi izleme\n",
        "        - Erken durdurma\n",
        "        - Daha detaylı metrikler\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "        # Öğrenme oranı çizelgesi\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=num_epochs\n",
        "        )\n",
        "\n",
        "        best_accuracy = 0.0\n",
        "        best_f1 = 0.0\n",
        "        patience = 3\n",
        "        patience_counter = 0\n",
        "\n",
        "        train_losses = []\n",
        "        val_accuracies = []\n",
        "        val_f1_scores = []\n",
        "\n",
        "        total_steps = num_epochs * len(train_loader)\n",
        "        current_step = 0\n",
        "\n",
        "        # Eğitim başlangıç zamanı\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            # TRAINING PHASE\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Eğitim Epoch {epoch+1}/{num_epochs}\")\n",
        "            for batch in progress_bar:\n",
        "                pixel_values = batch['pixel_values'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                # Eğitim doğruluğunu takip et\n",
        "                _, predicted = torch.max(outputs.logits, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                current_step += 1\n",
        "                progress_bar.set_postfix({'loss': loss.item()})\n",
        "                if progress is not None:\n",
        "                    progress((current_step / total_steps) * 100, f\"Epoch {epoch+1}/{num_epochs}, Batch {current_step % len(train_loader)}/{len(train_loader)}\")\n",
        "\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            train_losses.append(epoch_loss)\n",
        "            train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "            # VALIDATION PHASE\n",
        "            self.model.eval()\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(val_loader, desc=f\"Doğrulama Epoch {epoch+1}/{num_epochs}\"):\n",
        "                    pixel_values = batch['pixel_values'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
        "                    val_loss += outputs.loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Doğrulama metrikleri\n",
        "            val_epoch_loss = val_loss / len(val_loader)\n",
        "            accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "            val_accuracies.append(accuracy)\n",
        "\n",
        "            # F1 skoru hesapla\n",
        "            from sklearn.metrics import f1_score\n",
        "            f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "            val_f1_scores.append(f1)\n",
        "\n",
        "            # Doğrulama karışıklık matrisini hesapla\n",
        "            cm = confusion_matrix(all_labels, all_preds, labels=range(len(CLASSES)))\n",
        "\n",
        "            # Epoch sonuçlarını yazdır\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            log_msg = (f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "                      f'Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
        "                      f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {accuracy:.2f}%, '\n",
        "                      f'F1: {f1:.4f}, Time: {epoch_time:.1f}s')\n",
        "            print(log_msg)\n",
        "\n",
        "            # En iyi modeli kaydet\n",
        "            improvement = False\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                improvement = True\n",
        "                print(f\"✓ Doğruluk iyileşti: {best_accuracy:.2f}%\")\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                improvement = True\n",
        "                print(f\"✓ F1 skoru iyileşti: {best_f1:.4f}\")\n",
        "\n",
        "            if improvement:\n",
        "                model_path = os.path.join(os.getcwd(), '/content/drive/MyDrive/RVL_CDIP_small/best_document_classifier.pth')\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'train_loss': epoch_loss,\n",
        "                    'val_loss': val_epoch_loss,\n",
        "                    'accuracy': accuracy,\n",
        "                    'f1': f1\n",
        "                }, model_path)\n",
        "                print(f\"✓ En iyi model kaydedildi: {model_path}\")\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                print(f\"! İyileşme yok. Kalan sabır: {patience - patience_counter}\")\n",
        "\n",
        "            # Erken durdurma kontrolü\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"! {patience} epoch boyunca iyileşme olmadı. Eğitim erken durduruluyor.\")\n",
        "                break\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            # Belleği temizle\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Eğitim süresi\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Toplam eğitim süresi: {total_time/60:.2f} dakika\")\n",
        "\n",
        "        # Eğitim sonuçlarını grafikle\n",
        "        plot_path = self._plot_training_results(train_losses, val_accuracies, val_f1_scores, num_epochs)\n",
        "\n",
        "        return {\n",
        "            'train_losses': train_losses,\n",
        "            'val_accuracies': val_accuracies,\n",
        "            'val_f1_scores': val_f1_scores,\n",
        "            'best_accuracy': best_accuracy,\n",
        "            'best_f1': best_f1,\n",
        "            'plot_path': plot_path,\n",
        "            'total_time': total_time\n",
        "        }\n",
        "\n",
        "    def _plot_training_results(self, train_losses, val_accuracies, val_f1_scores, num_epochs):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Kayıp grafiği\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Eğitim Kaybı')\n",
        "        plt.title('Eğitim Kaybı')\n",
        "        plt.xlabel('Epok')\n",
        "        plt.ylabel('Kayıp')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.legend()\n",
        "\n",
        "        # Doğruluk grafiği\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(range(1, len(val_accuracies)+1), val_accuracies, 'r-', label='Doğrulama Doğruluğu')\n",
        "        plt.title('Doğrulama Doğruluğu')\n",
        "        plt.xlabel('Epok')\n",
        "        plt.ylabel('Doğruluk (%)')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.legend()\n",
        "\n",
        "        # F1 skor grafiği\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(range(1, len(val_f1_scores)+1), val_f1_scores, 'g-', label='F1 Skoru')\n",
        "        plt.title('Doğrulama F1 Skoru')\n",
        "        plt.xlabel('Epok')\n",
        "        plt.ylabel('F1 Skoru')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Geçici dosya oluştur\n",
        "        plot_path = os.path.join(os.getcwd(), 'training_results.png')\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        return plot_path\n",
        "\n",
        "    def load_best_model(self, model_path='/content/drive/MyDrive/RVL_CDIP_small/best_document_classifier.pth'):\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "            # Eski model kayıt formatı kontrolü (state_dict doğrudan kaydedildiyse)\n",
        "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(f\"Model yüklendi: {model_path} (Epoch: {checkpoint.get('epoch', 'Bilinmiyor')})\")\n",
        "                print(f\"Doğruluk: {checkpoint.get('accuracy', 'Bilinmiyor'):.2f}%, F1: {checkpoint.get('f1', 'Bilinmiyor'):.4f}\")\n",
        "            else:\n",
        "                self.model.load_state_dict(checkpoint)\n",
        "                print(f\"Model yüklendi: {model_path} (Eski format)\")\n",
        "\n",
        "            self.model.eval()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Model yüklenirken hata oluştu: {e}\")\n",
        "            return False\n",
        "\n",
        "    def predict(self, image_path):\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "            pixel_values = inputs[\"pixel_values\"].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(pixel_values=pixel_values)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "\n",
        "                # En yüksek olasılığa sahip sınıfı ve güveni al\n",
        "                confidence, predicted_class_idx = torch.max(probs, 1)\n",
        "                predicted_class = self.idx_to_class[predicted_class_idx.item()]\n",
        "                confidence = confidence.item()\n",
        "\n",
        "                # Tüm sınıf olasılıklarını al\n",
        "                class_probs = {self.idx_to_class[i]: prob.item() for i, prob in enumerate(probs[0])}\n",
        "\n",
        "                # Olasılıkları sırala\n",
        "                sorted_probs = sorted(class_probs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                return {\n",
        "                    'class': predicted_class,\n",
        "                    'confidence': confidence,\n",
        "                    'all_probs': class_probs,\n",
        "                    'sorted_probs': sorted_probs\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"Tahmin hatası: {e}\")\n",
        "            return {\n",
        "                'class': 'error',\n",
        "                'confidence': 0.0,\n",
        "                'error': str(e),\n",
        "                'all_probs': {},\n",
        "                'sorted_probs': []\n",
        "            }\n",
        "\n",
        "    def evaluate(self, test_loader, progress=None):\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_batches = len(test_loader)\n",
        "            for i, batch in enumerate(tqdm(test_loader, desc=\"Değerlendirme\")):\n",
        "                pixel_values = batch['pixel_values'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(pixel_values=pixel_values)\n",
        "                logits = outputs.logits\n",
        "                probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "                _, predicted = torch.max(logits, 1)\n",
        "\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "                # İlerleme durumunu güncelle\n",
        "                if progress is not None:\n",
        "                    progress((i / total_batches) * 100, f\"Değerlendiriliyor: Batch {i+1}/{total_batches}\")\n",
        "\n",
        "        # Sınıflandırma raporu oluştur\n",
        "        report = classification_report(\n",
        "            all_labels,\n",
        "            all_preds,\n",
        "            target_names=CLASSES,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        # Karışıklık matrisi oluştur\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        # ROC eğrisi için gerekli hesaplamaları yap\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "        y_test_bin = label_binarize(all_labels, classes=range(len(CLASSES)))\n",
        "\n",
        "        # Grafikleri oluştur\n",
        "        cm_plot_path, f1_plot_path, roc_plot_path = self._plot_evaluation_results(cm, report, all_labels, all_probs)\n",
        "\n",
        "        # Hatalı sınıflandırılan örnekleri incele\n",
        "        misclassified = np.where(np.array(all_preds) != np.array(all_labels))[0]\n",
        "\n",
        "        return {\n",
        "            'report': report,\n",
        "            'confusion_matrix': cm,\n",
        "            'cm_plot_path': cm_plot_path,\n",
        "            'f1_plot_path': f1_plot_path,\n",
        "            'roc_plot_path': roc_plot_path,\n",
        "            'misclassified_indices': misclassified\n",
        "        }\n",
        "\n",
        "    def _plot_evaluation_results(self, cm, report, all_labels, all_probs):\n",
        "        # 1. Karışıklık Matrisi\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "        plt.title('Karmaşıklık Matrisi')\n",
        "        plt.xlabel('Tahmin Edilen Etiket')\n",
        "        plt.ylabel('Gerçek Etiket')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        cm_plot_path = os.path.join(os.getcwd(), 'confusion_matrix.png')\n",
        "        plt.savefig(cm_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        # 2. F1 Skorları\n",
        "        df_report = pd.DataFrame(report).transpose()\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bar_plot = sns.barplot(x=df_report.index[:-3], y=df_report['f1-score'][:-3])\n",
        "\n",
        "        # Değerleri çubukların üzerine ekle\n",
        "        for i, p in enumerate(bar_plot.patches):\n",
        "            bar_plot.annotate(f\"{p.get_height():.2f}\",\n",
        "                       (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                       ha = 'center', va = 'bottom',\n",
        "                       xytext = (0, 5),\n",
        "                       textcoords = 'offset points')\n",
        "\n",
        "        plt.title('Sınıf Bazında F1 Skorları')\n",
        "        plt.xlabel('Belge Sınıfı')\n",
        "        plt.ylabel('F1 Skoru')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        f1_plot_path = os.path.join(os.getcwd(), 'f1_scores.png')\n",
        "        plt.savefig(f1_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        # 3. ROC Eğrisi (Bire-karşı-hepsi yaklaşımı)\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "        # Çok sınıflı ROC analizi - Her sınıfın bireysel ROC eğrisi\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        y_bin = label_binarize(all_labels, classes=range(len(CLASSES)))\n",
        "        n_classes = y_bin.shape[1]\n",
        "\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "\n",
        "        # Her sınıf için sadece birkaç sınıfı çiz (16 sınıfın hepsi grafiği karmaşık yapabilir)\n",
        "        classes_to_plot = min(8, len(CLASSES))\n",
        "        for i in range(classes_to_plot):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], np.array(all_probs)[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                     label=f'{CLASSES[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('Yanlış Pozitif Oranı')\n",
        "        plt.ylabel('Doğru Pozitif Oranı')\n",
        "        plt.title('Seçilmiş Sınıflar için ROC Eğrisi')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        roc_plot_path = os.path.join(os.getcwd(), 'roc_curves.png')\n",
        "        plt.savefig(roc_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        return cm_plot_path, f1_plot_path, roc_plot_path"
      ],
      "metadata": {
        "id": "RmNqjSGf9BDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek LLM ile analiz sınıfı\n",
        "class DocumentAnalyzer:\n",
        "    def __init__(self, model=\"deepseek-ai/deepseek-llm-7b-chat\"):\n",
        "        self.model = model\n",
        "        self.timeout = 120  # LLM analizi için 120 saniyelik zaman aşımı\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Yerel model pipeline'ı\n",
        "            self.pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "            print(f\"DeepSeek LLM modeli başarıyla yüklendi: {model}\")\n",
        "        except Exception as e:\n",
        "            print(f\"DeepSeek LLM modeli yüklenirken hata oluştu: {e}\")\n",
        "            self.pipe = None\n",
        "\n",
        "    def analyze_document(self, text, document_class, prompt_template=None):\n",
        "        \"\"\"\n",
        "        Belge metnini analiz et ve önemli bilgileri çıkar\n",
        "        Gelişmiş hata yakalama ve zaman aşımı kontrolü içerir\n",
        "        \"\"\"\n",
        "        import threading\n",
        "        import time\n",
        "        from queue import Queue\n",
        "\n",
        "        # Sonuç ve hata kuyrukları\n",
        "        result_queue = Queue()\n",
        "        exception_queue = Queue()\n",
        "\n",
        "        # Prompt şablonunu belirle\n",
        "        if prompt_template is None:\n",
        "            # Sınıf-spesifik özel şablonlar\n",
        "            class_specific_templates = {\n",
        "                'letter': \"\"\"\n",
        "                Bu belge \"letter\" (mektup) olarak sınıflandırılmıştır.\n",
        "                Lütfen aşağıdaki mektubu analiz et ve şu bilgileri çıkar:\n",
        "\n",
        "                1. Mektubun kimden geldiği ve kime hitap ettiği\n",
        "                2. Mektubun yazılış tarihi\n",
        "                3. Mektubun ana konusu ve amacı\n",
        "                4. Mektupta belirtilen önemli bilgiler\n",
        "                5. Mektupta bahsedilen kişiler, kurumlar veya yerler\n",
        "                6. Mektubun tonu ve üslubu (resmi, gayri resmi, iş mektubu vb.)\n",
        "                7. Mektubun içeriğinin kısa bir özeti\n",
        "\n",
        "                Mektup İçeriği:\n",
        "                {text}\n",
        "                \"\"\",\n",
        "\n",
        "                'form': \"\"\"\n",
        "                Bu belge \"form\" (form) olarak sınıflandırılmıştır.\n",
        "                Lütfen aşağıdaki formu analiz et ve şu bilgileri çıkar:\n",
        "\n",
        "                1. Formun türü ve amacı\n",
        "                2. Formda bulunan ana bölümler\n",
        "                3. Formdaki önemli alanlar ve bilgiler (varsa)\n",
        "                4. Form ile ilgili kurumsal bilgiler\n",
        "                5. Formun doldurulması için gereken bilgiler\n",
        "                6. Formun genel yapısı ve organizasyonu\n",
        "                7. Formun içeriğinin kısa bir özeti\n",
        "\n",
        "                Form İçeriği:\n",
        "                {text}\n",
        "                \"\"\",\n",
        "\n",
        "                'invoice': \"\"\"\n",
        "                Bu belge \"invoice\" (fatura) olarak sınıflandırılmıştır.\n",
        "                Lütfen aşağıdaki faturayı analiz et ve şu bilgileri çıkar:\n",
        "\n",
        "                1. Fatura tarihi ve numarası\n",
        "                2. Satıcı/sağlayıcı bilgileri\n",
        "                3. Alıcı bilgileri\n",
        "                4. Fatura kalemleri (ürünler/hizmetler ve fiyatları)\n",
        "                5. Toplam tutar, vergi tutarı ve varsa indirimler\n",
        "                6. Ödeme koşulları ve son ödeme tarihi\n",
        "                7. Faturanın içeriğinin kısa bir özeti\n",
        "\n",
        "                Fatura İçeriği:\n",
        "                {text}\n",
        "                \"\"\",\n",
        "\n",
        "                'email': \"\"\"\n",
        "                Bu belge \"email\" (e-posta) olarak sınıflandırılmıştır.\n",
        "                Lütfen aşağıdaki e-postayı analiz et ve şu bilgileri çıkar:\n",
        "\n",
        "                1. E-postanın kimden geldiği ve kime gönderildiği\n",
        "                2. E-postanın tarihi ve saati\n",
        "                3. E-postanın konusu\n",
        "                4. E-postada belirtilen önemli bilgiler veya talepler\n",
        "                5. E-postada bahsedilen kişiler, kurumlar veya projeler\n",
        "                6. E-postanın tonu ve amacı\n",
        "                7. E-postanın içeriğinin kısa bir özeti\n",
        "\n",
        "                E-posta İçeriği:\n",
        "                {text}\n",
        "                \"\"\"\n",
        "            }\n",
        "\n",
        "            # Sınıfa özel şablon varsa kullan, yoksa genel şablonu kullan\n",
        "            prompt_template = class_specific_templates.get(document_class, \"\"\"\n",
        "            Bu belge \"{document_class}\" olarak sınıflandırılmıştır.\n",
        "            Lütfen aşağıdaki belgeyi analiz et ve şu bilgileri çıkar:\n",
        "\n",
        "            1. Belge tipi ve amacı nedir?\n",
        "            2. Ana konusu veya içeriği nedir?\n",
        "            3. Önemli anahtar-değer çiftleri (varsa)\n",
        "            4. Belgedeki önemli varlıklar (kişiler, şirketler, tarihler vb.)\n",
        "            5. Belgenin yapısı ve formatı nasıldır?\n",
        "            6. Belgenin hedef kitlesi kimdir?\n",
        "            7. Belgenin içeriğinin özeti\n",
        "\n",
        "            Belge İçeriği:\n",
        "            {text}\n",
        "            \"\"\")\n",
        "\n",
        "        # Analiz çalışanı\n",
        "        def analyze_worker():\n",
        "            try:\n",
        "                # Analiz başlangıç zamanı\n",
        "                start_time = time.time()\n",
        "                print(f\"LLM analizi başlıyor, metin uzunluğu: {len(text)} karakter\")\n",
        "\n",
        "                # Metni kısalt (çok uzunsa)\n",
        "                max_chars = 500   # Maksimum 4000 karakter\n",
        "                truncated_text = text[:max_chars] if len(text) > max_chars else text\n",
        "\n",
        "                # Prompt oluştur\n",
        "                prompt = prompt_template.format(document_class=document_class, text=truncated_text)\n",
        "\n",
        "                # LLM yanıtı al\n",
        "                if self.pipe is None:\n",
        "                    result_queue.put({\n",
        "                        'analysis': \"LLM modeli yüklenemediği için analiz yapılamadı.\",\n",
        "                        'model': \"None\",\n",
        "                        'error': \"Model yüklenemedi\"\n",
        "                    })\n",
        "                    return\n",
        "\n",
        "                response = self.pipe(\n",
        "                    prompt,\n",
        "                    max_new_tokens=800,\n",
        "                    temperature=0.1,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "                generated_text = response[0]['generated_text']\n",
        "\n",
        "                # Prompt'u çıkar ve sadece yanıtı al\n",
        "                response_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "                # İşlem süresini hesapla\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"LLM analizi tamamlandı: {len(response_text)} karakter, {elapsed_time:.2f} saniye\")\n",
        "\n",
        "                # Sonucu kuyruğa ekle\n",
        "                result_queue.put({\n",
        "                    'analysis': response_text,\n",
        "                    'model': self.model,\n",
        "                    'processing_time': elapsed_time\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"LLM analiz hatası: {e}\")\n",
        "                exception_queue.put(e)\n",
        "\n",
        "        # İş parçacığını başlat\n",
        "        thread = threading.Thread(target=analyze_worker)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "        # Zaman aşımı ile bekle\n",
        "        thread.join(timeout=self.timeout)\n",
        "\n",
        "        # Thread hala çalışıyorsa zaman aşımına uğradı\n",
        "        if thread.is_alive():\n",
        "            print(f\"LLM analizi zaman aşımına uğradı ({self.timeout} saniye)\")\n",
        "            return {\n",
        "                'analysis': f\"[LLM analizi zaman aşımına uğradı ({self.timeout} saniye). Belge çok karmaşık veya LLM işlem kapasitesini aşıyor olabilir.]\",\n",
        "                'model': self.model,\n",
        "                'error': 'timeout'\n",
        "            }\n",
        "\n",
        "        # İstisna olup olmadığını kontrol et\n",
        "        if not exception_queue.empty():\n",
        "            e = exception_queue.get()\n",
        "            return {\n",
        "                'analysis': f\"[LLM analiz hatası: {str(e)}]\",\n",
        "                'model': self.model,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "        # Sonuçları döndür\n",
        "        if not result_queue.empty():\n",
        "            return result_queue.get()\n",
        "        else:\n",
        "            return {\n",
        "                'analysis': \"[LLM analizi tamamlandı ancak sonuç bulunamadı]\",\n",
        "                'model': self.model,\n",
        "                'error': 'no_result'\n",
        "            }"
      ],
      "metadata": {
        "id": "KEHoXzNG9EDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessingPipeline:\n",
        "    def __init__(self, data_dir=None, model_dir=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.model_dir = model_dir or os.path.join(os.getcwd(), 'models')\n",
        "\n",
        "        # Çıktı dizinlerini oluştur\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "        print(\"Belge işleme pipeline'ı başlatılıyor...\")\n",
        "\n",
        "        # Sınıflandırıcıyı oluştur\n",
        "        print(\"Swin Transformer modelini yüklüyor...\")\n",
        "        self.classifier = DocumentClassifier()\n",
        "\n",
        "        # Eğitilmiş modeli yükle (varsa)\n",
        "        best_model_path = os.path.join(os.getcwd(),'/content/drive/MyDrive/RVL_CDIP_small/best_document_classifier.pth')\n",
        "        if os.path.exists(best_model_path):\n",
        "            self.classifier.load_best_model(best_model_path)\n",
        "        else:\n",
        "            print(\"Hazır model bulunamadı. Yeni model eğitimi gerekecek.\")\n",
        "\n",
        "        print(\"Metin çıkarıcıyı başlatıyor...\")\n",
        "        self.extractor = DocumentTextExtractor()\n",
        "\n",
        "        print(\"Belge analizörünü başlatıyor...\")\n",
        "        self.analyzer = DocumentAnalyzer()\n",
        "\n",
        "        print(\"Pipeline hazır!\")\n",
        "\n",
        "    def train_classifier(self, train_dir=None, val_dir=None, batch_size=16, num_epochs=10, learning_rate=2e-5, progress_callback=None):\n",
        "        print(f\"Sınıflandırıcı eğitimi başlıyor...\")\n",
        "\n",
        "        # Eğitim ve doğrulama dizinlerini kontrol et\n",
        "        train_dir = train_dir or os.path.join(self.data_dir, 'train')\n",
        "        val_dir = val_dir or os.path.join(self.data_dir, 'val')\n",
        "\n",
        "        if not os.path.exists(train_dir) or not os.path.exists(val_dir):\n",
        "            print(\"Eğitim ve doğrulama dizinleri bulunamadı. Veri seti hazırlanıyor...\")\n",
        "\n",
        "            # Veri seti yapısını kontrol et\n",
        "            has_subfolders = any(os.path.isdir(os.path.join(self.data_dir, d)) and d in CLASSES for d in os.listdir(self.data_dir))\n",
        "\n",
        "            if has_subfolders:\n",
        "                # Veri setini hazırla\n",
        "                train_dir, val_dir, _ = prepare_dataset(self.data_dir)\n",
        "            else:\n",
        "                print(\"UYARI: Veri seti formatı doğru değil, model eğitimi başarısız olabilir!\")\n",
        "                train_dir = val_dir = self.data_dir\n",
        "\n",
        "        # Veri artırma dönüşümlerini al\n",
        "        train_transform = self.classifier.processor.get_train_transform()\n",
        "\n",
        "        # Veri setlerini oluştur\n",
        "        train_dataset = DocumentDataset(train_dir, self.classifier.processor, transform=train_transform, is_training=True)\n",
        "        val_dataset = DocumentDataset(val_dir, self.classifier.processor)\n",
        "\n",
        "        # Veri yükleyicileri oluştur\n",
        "        num_workers = min(os.cpu_count() or 1, 4)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "        print(f\"Eğitim veri seti: {len(train_dataset)} belge\")\n",
        "        print(f\"Doğrulama veri seti: {len(val_dataset)} belge\")\n",
        "        print(f\"Batch boyutu: {batch_size}, Epoch sayısı: {num_epochs}, İşçi sayısı: {num_workers}\")\n",
        "\n",
        "        # Belleği temizle\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Modeli eğit\n",
        "        results = self.classifier.train(\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            num_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            progress=progress_callback\n",
        "        )\n",
        "\n",
        "        # Sonuçları kaydet\n",
        "        results_file = os.path.join(self.model_dir, 'training_results.json')\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump({\n",
        "                'train_losses': [float(loss) for loss in results['train_losses']],\n",
        "                'val_accuracies': [float(acc) for acc in results['val_accuracies']],\n",
        "                'val_f1_scores': [float(f1) for f1 in results['val_f1_scores']],\n",
        "                'best_accuracy': float(results['best_accuracy']),\n",
        "                'best_f1': float(results['best_f1']),\n",
        "                'training_time_seconds': float(results['total_time'])\n",
        "            }, f, indent=2)\n",
        "\n",
        "        print(f\"Eğitim sonuçları kaydedildi: {results_file}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_classifier(self, test_dir=None, batch_size=16, progress_callback=None):\n",
        "        print(\"Sınıflandırıcı değerlendirmesi başlıyor...\")\n",
        "\n",
        "        test_dir = test_dir or os.path.join(self.data_dir, 'test')\n",
        "\n",
        "        # Test dizinini kontrol et\n",
        "        if not os.path.exists(test_dir) and os.path.exists(os.path.join(self.data_dir, 'train')):\n",
        "            print(\"Test dizini bulunamadı. Veri seti yeniden hazırlanıyor...\")\n",
        "            _, _, test_dir = prepare_dataset(self.data_dir)\n",
        "        elif not os.path.exists(test_dir):\n",
        "            # Test dizini yoksa, ana veri seti dizinini kullan\n",
        "            test_dir = self.data_dir\n",
        "            print(f\"Test dizini bulunamadı, ana veri seti dizini kullanılıyor: {test_dir}\")\n",
        "\n",
        "        # Test veri setini oluştur\n",
        "        test_dataset = DocumentDataset(test_dir, self.classifier.processor)\n",
        "\n",
        "        # Test veri yükleyicisini oluştur\n",
        "        num_workers = min(os.cpu_count() or 1, 4)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "        print(f\"Test veri seti: {len(test_dataset)} belge\")\n",
        "\n",
        "        # Modeli değerlendir\n",
        "        eval_results = self.classifier.evaluate(test_loader, progress=progress_callback)\n",
        "\n",
        "        # Raporu kaydet\n",
        "        report_df = pd.DataFrame(eval_results['report']).transpose()\n",
        "        report_path = os.path.join(self.model_dir, 'classification_report.csv')\n",
        "        report_df.to_csv(report_path)\n",
        "\n",
        "        print(f\"Değerlendirme raporu kaydedildi: {report_path}\")\n",
        "\n",
        "        return eval_results\n",
        "\n",
        "    def process_document(self, document_path, skip_analysis=False):\n",
        "        \"\"\"\n",
        "        Belge işleme pipeline'ını çalıştır:\n",
        "        1. Görsel sınıflandırma\n",
        "        2. Metin çıkarma\n",
        "        3. İçerik analizi (gerekirse)\n",
        "        \"\"\"\n",
        "        print(f\"Belge işleniyor: {document_path}\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Adım 1: Görsel sınıflandırma\n",
        "        print(\"Adım 1: Görsel sınıflandırma...\")\n",
        "        try:\n",
        "            classification_result = self.classifier.predict(document_path)\n",
        "            results['classification'] = classification_result\n",
        "\n",
        "            doc_class = classification_result['class']\n",
        "            confidence = classification_result['confidence']\n",
        "            print(f\"Belge sınıfı: {doc_class} (güven: {confidence:.4f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Sınıflandırma hatası: {e}\")\n",
        "            results['classification'] = {\n",
        "                'class': 'error',\n",
        "                'confidence': 0.0,\n",
        "                'error': str(e)\n",
        "            }\n",
        "            doc_class = 'error'\n",
        "            confidence = 0.0\n",
        "\n",
        "        # Adım 2: Metin çıkarma (zaman aşımı koruması ile)\n",
        "        print(\"Adım 2: Metin çıkarma...\")\n",
        "        try:\n",
        "            # Zaman aşımı için bir süre belirle\n",
        "            extraction_timeout = 30  # 30 saniye\n",
        "\n",
        "            # Thread kullanarak zaman aşımını kontrol et\n",
        "            def extract_with_timeout():\n",
        "                return self.extractor.extract_text(document_path)\n",
        "\n",
        "            import threading\n",
        "            extraction_thread = threading.Thread(target=extract_with_timeout)\n",
        "            extraction_thread.daemon = True\n",
        "\n",
        "            extraction_result = None\n",
        "            extraction_thread.start()\n",
        "            extraction_thread.join(extraction_timeout)\n",
        "\n",
        "            if extraction_thread.is_alive():\n",
        "                print(f\"Metin çıkarma zaman aşımına uğradı ({extraction_timeout} saniye)\")\n",
        "                # Zaman aşımına uğrarsa, varsayılan boş sonuç döndür\n",
        "                extraction_result = {\n",
        "                    'text': \"[Metin çıkarma zaman aşımına uğradı]\",\n",
        "                    'metadata': {'error': 'timeout'}\n",
        "                }\n",
        "            else:\n",
        "                extraction_result = extract_with_timeout()\n",
        "\n",
        "            results['extraction'] = {\n",
        "                'text': extraction_result['text'],\n",
        "                'metadata': extraction_result['metadata']\n",
        "            }\n",
        "\n",
        "            text_length = len(extraction_result['text'])\n",
        "            print(f\"Çıkarılan metin uzunluğu: {text_length} karakter\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Metin çıkarma hatası: {e}\")\n",
        "            results['extraction'] = {\n",
        "                'text': \"[Metin çıkarma başarısız oldu]\",\n",
        "                'metadata': {'error': str(e)}\n",
        "            }\n",
        "            text_length = 0\n",
        "\n",
        "        # Adım 3: İçerik analizi (opsiyonel)\n",
        "        if not skip_analysis and confidence > 0.5 and text_length > 50:\n",
        "            print(\"Adım 3: İçerik analizi...\")\n",
        "            try:\n",
        "                analysis_result = self.analyzer.analyze_document(\n",
        "                    results['extraction']['text'],\n",
        "                    doc_class\n",
        "                )\n",
        "                results['analysis'] = analysis_result\n",
        "                print(\"İçerik analizi tamamlandı.\")\n",
        "            except Exception as e:\n",
        "                print(f\"İçerik analizi hatası: {e}\")\n",
        "                results['analysis'] = {\n",
        "                    'analysis': \"[Analiz hatası oluştu]\",\n",
        "                    'error': str(e)\n",
        "                }\n",
        "        else:\n",
        "            if skip_analysis:\n",
        "                print(\"İçerik analizi atlandı (kullanıcı tercihiyle).\")\n",
        "            else:\n",
        "                print(\"İçerik analizi atlandı (düşük güven veya yetersiz metin).\")\n",
        "            results['analysis'] = None\n",
        "\n",
        "        print(\"Belge işleme tamamlandı.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_batch(self, folder_path, max_documents=None, output_csv=None, skip_analysis=False, progress_callback=None):\n",
        "        \"\"\"\n",
        "        Bir klasördeki belgeleri toplu olarak işle\n",
        "        \"\"\"\n",
        "        print(f\"Toplu işleme başlatılıyor: {folder_path}\")\n",
        "\n",
        "        # Tüm uygun belgeleri bul\n",
        "        documents = []\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.endswith(('.tif', '.jpg', '.png', '.jpeg', '.pdf')):\n",
        "                    documents.append(os.path.join(root, file))\n",
        "\n",
        "        # Maksimum belge sayısını kontrol et\n",
        "        if max_documents and len(documents) > max_documents:\n",
        "            documents = documents[:max_documents]\n",
        "\n",
        "        print(f\"İşlenecek belge sayısı: {len(documents)}\")\n",
        "\n",
        "        # Her belgeyi işle\n",
        "        results = []\n",
        "\n",
        "        for i, doc_path in enumerate(tqdm(documents, desc=\"Belgeler işleniyor\")):\n",
        "            if progress_callback:\n",
        "                progress_callback((i / len(documents)) * 100, f\"Belge işleniyor: {i+1}/{len(documents)}\")\n",
        "\n",
        "            try:\n",
        "                result = self.process_document(doc_path, skip_analysis=skip_analysis)\n",
        "\n",
        "                # Sonuçları kaydet\n",
        "                results.append({\n",
        "                    'path': doc_path,\n",
        "                    'class': result['classification']['class'],\n",
        "                    'confidence': result['classification']['confidence'],\n",
        "                    'text_length': len(result['extraction']['text']),\n",
        "                    'analysis': result['analysis']['analysis'] if result['analysis'] else None\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Hata: {e}\")\n",
        "                results.append({\n",
        "                    'path': doc_path,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "        # Sonuçları CSV olarak kaydet\n",
        "        if output_csv:\n",
        "            df = pd.DataFrame(results)\n",
        "            df.to_csv(output_csv, index=False)\n",
        "            print(f\"Sonuçlar kaydedildi: {output_csv}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "x1FumdM59H_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnstructuredTextExtractor:\n",
        "    def __init__(self):\n",
        "        print(\"Unstructured metin çıkarıcı başlatılıyor...\")\n",
        "        # Zaman aşımı ekleyebiliriz\n",
        "        self.timeout = 60  # 60 saniye\n",
        "\n",
        "    def extract_text(self, document_path):\n",
        "        \"\"\"\n",
        "        Unstructured kütüphanesi ile metin çıkarma işlemi\n",
        "        Güvenlik için zaman aşımı ve hata yönetimi eklenmiştir\n",
        "        \"\"\"\n",
        "        import threading\n",
        "        import time\n",
        "        from queue import Queue\n",
        "\n",
        "        result_queue = Queue()\n",
        "        exception_queue = Queue()\n",
        "\n",
        "        def extraction_worker():\n",
        "            try:\n",
        "                # Hata ayıklama için başlangıç zamanı\n",
        "                start_time = time.time()\n",
        "                print(f\"Metin çıkarma başlıyor: {document_path}\")\n",
        "\n",
        "                # Belgenin uzantısını kontrol et\n",
        "                ext = os.path.splitext(document_path)[1].lower()\n",
        "\n",
        "                # TIF dosyalarına özel işlem\n",
        "                if ext in ['.tif', '.tiff']:\n",
        "                    try:\n",
        "                        # TIF dosyasını PIL ile açıp geçici bir JPEG olarak kaydet\n",
        "                        from PIL import Image\n",
        "                        print(f\"TIF dosyası işleniyor: {document_path}\")\n",
        "                        img = Image.open(document_path)\n",
        "\n",
        "                        # Geçici jpeg dosyası oluştur\n",
        "                        temp_jpg = document_path + \"_temp.jpg\"\n",
        "                        img.convert('RGB').save(temp_jpg)\n",
        "                        print(f\"Geçici JPEG oluşturuldu: {temp_jpg}\")\n",
        "\n",
        "                        # Geçici JPEG dosyasını işle\n",
        "                        from unstructured.partition.auto import partition\n",
        "                        from unstructured.staging.base import elements_to_text\n",
        "\n",
        "                        print(\"Unstructured ile ayrıştırma başlıyor...\")\n",
        "                        elements = partition(temp_jpg)\n",
        "                        text = elements_to_text(elements)\n",
        "\n",
        "                        # Geçici dosyayı temizle\n",
        "                        try:\n",
        "                            os.remove(temp_jpg)\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"TIF işleme hatası: {e}\")\n",
        "                        text = f\"[TIF işleme hatası: {str(e)}]\"\n",
        "                        elements = []\n",
        "                else:\n",
        "                    # Diğer dosya türleri için doğrudan partition kullan\n",
        "                    try:\n",
        "                        from unstructured.partition.auto import partition\n",
        "                        from unstructured.staging.base import elements_to_text\n",
        "\n",
        "                        print(\"Unstructured ile ayrıştırma başlıyor...\")\n",
        "                        elements = partition(document_path)\n",
        "                        text = elements_to_text(elements)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Ayrıştırma hatası: {e}\")\n",
        "                        text = f\"[Ayrıştırma hatası: {str(e)}]\"\n",
        "                        elements = []\n",
        "\n",
        "                # İşlem süresini kontrol et\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Metin çıkarma tamamlandı: {len(text)} karakter, {elapsed_time:.2f} saniye\")\n",
        "\n",
        "                # Sonuçları kuyruğa ekle\n",
        "                result_queue.put({\n",
        "                    'text': text,\n",
        "                    'metadata': {\n",
        "                        'num_elements': len(elements),\n",
        "                        'processing_time': elapsed_time,\n",
        "                        'file_path': document_path\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Metin çıkarma thread hatası: {e}\")\n",
        "                exception_queue.put(e)\n",
        "\n",
        "        # İş parçacığını başlat\n",
        "        thread = threading.Thread(target=extraction_worker)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "        # Zaman aşımı ile bekle\n",
        "        thread.join(timeout=self.timeout)\n",
        "\n",
        "        # Thread hala çalışıyorsa zaman aşımına uğradı\n",
        "        if thread.is_alive():\n",
        "            print(f\"Metin çıkarma zaman aşımına uğradı ({self.timeout} saniye)\")\n",
        "            return {\n",
        "                'text': f\"[Metin çıkarma zaman aşımına uğradı ({self.timeout} saniye)]\",\n",
        "                'metadata': {'error': 'timeout', 'file_path': document_path}\n",
        "            }\n",
        "\n",
        "        # İstisna olup olmadığını kontrol et\n",
        "        if not exception_queue.empty():\n",
        "            e = exception_queue.get()\n",
        "            return {\n",
        "                'text': f\"[Metin çıkarma hatası: {str(e)}]\",\n",
        "                'metadata': {'error': str(e), 'file_path': document_path}\n",
        "            }\n",
        "\n",
        "        # Sonuçları döndür\n",
        "        if not result_queue.empty():\n",
        "            return result_queue.get()\n",
        "        else:\n",
        "            return {\n",
        "                'text': \"[Metin çıkarma işlemi tamamlandı ancak sonuç bulunamadı]\",\n",
        "                'metadata': {'error': 'no_result', 'file_path': document_path}\n",
        "            }\n"
      ],
      "metadata": {
        "id": "q8oG4OhC9Pfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DocumentProcessingUI:\n",
        "    def __init__(self, data_dir=None):\n",
        "        # Varsayılan model dosyası\n",
        "        self.model_path = os.path.join(os.getcwd(), '/content/drive/MyDrive/RVL_CDIP_small/best_document_classifier.pth')\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Sınıflandırıcıyı başlat\n",
        "        self.classifier = DocumentClassifier()\n",
        "\n",
        "        # OCR işleyicisini başlat\n",
        "        self.extractor = UnstructuredTextExtractor()\n",
        "\n",
        "        # LLM analiz modülünü başlat\n",
        "        self.analyzer = DocumentAnalyzer()\n",
        "\n",
        "        # Eğitilmiş model var mı kontrol et\n",
        "        self.model_exists = os.path.exists(self.model_path)\n",
        "\n",
        "        if self.model_exists:\n",
        "            success = self.classifier.load_best_model(self.model_path)\n",
        "            print(f\"Model yükleme {'başarılı' if success else 'başarısız'}: {self.model_path}\")\n",
        "        else:\n",
        "            print(f\"UYARI: Eğitilmiş model bulunamadı: {self.model_path}\")\n",
        "\n",
        "        # Arayüzü oluştur\n",
        "        self.app = self._create_ui()\n",
        "\n",
        "    def _create_ui(self):\n",
        "        with gr.Blocks(title=\"Belge İşleme Sistemi\", theme=gr.themes.Soft()) as app:\n",
        "            gr.Markdown(\"# 📄 Belge İşleme Sistemi\")\n",
        "            gr.Markdown(\"Swin Transformer ile belge sınıflandırma, OCR ve LLM analizi.\")\n",
        "\n",
        "            # Model durumunu göster\n",
        "            if self.model_exists:\n",
        "                gr.Markdown(f\"✅ Eğitilmiş model otomatik olarak yüklendi: {self.model_path}\")\n",
        "            else:\n",
        "                gr.Markdown(f\"⚠️ Eğitilmiş model bulunamadı. Lütfen önce modeli eğitin.\")\n",
        "\n",
        "            with gr.Tab(\"Belge İşleme\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        # Belge yükleme bölümü\n",
        "                        file_input = gr.File(label=\"Belge Seçin (PDF, TIF, JPG, PNG)\")\n",
        "                        with gr.Row():\n",
        "                            classify_btn = gr.Button(\"Sadece Sınıflandır\", variant=\"secondary\")\n",
        "                            process_btn = gr.Button(\"Sınıflandır & OCR\", variant=\"secondary\")\n",
        "                            analyze_btn = gr.Button(\"Tam Analiz (Sınıflandırma+OCR+LLM)\", variant=\"primary\")\n",
        "\n",
        "                        image_preview = gr.Image(type=\"filepath\", label=\"Belge Önizleme\")\n",
        "\n",
        "                    with gr.Column(scale=1):\n",
        "                        # Sonuç gösterimi bölümü\n",
        "                        result_box = gr.Textbox(label=\"Sonuç Özeti\", interactive=False, lines=2)\n",
        "                        doc_class = gr.Textbox(label=\"Belge Sınıfı\", interactive=False)\n",
        "                        confidence = gr.Number(label=\"Doğruluk Oranı (%)\", interactive=False)\n",
        "\n",
        "                        # Sınıf olasılıkları için tablo\n",
        "                        gr.Markdown(\"### Sınıf Olasılıkları\")\n",
        "                        top_classes = gr.Dataframe(\n",
        "                            headers=[\"Sınıf\", \"Olasılık (%)\"],\n",
        "                            datatype=[\"str\", \"number\"],\n",
        "                            row_count=(5, \"fixed\"),\n",
        "                            col_count=(2, \"fixed\"),\n",
        "                            interactive=False\n",
        "                        )\n",
        "\n",
        "                        # OCR Sonuçları için metin kutusu\n",
        "                        with gr.Accordion(\"Çıkarılan Metin (OCR)\", open=False):\n",
        "                            extracted_text = gr.Textbox(\n",
        "                                label=\"Belge Metni\",\n",
        "                                interactive=False,\n",
        "                                lines=10,\n",
        "                                placeholder=\"Metin çıkarma sonuçları burada görünecek.\"\n",
        "                            )\n",
        "                            ocr_info = gr.Textbox(\n",
        "                                label=\"OCR Bilgisi\",\n",
        "                                interactive=False,\n",
        "                                lines=1\n",
        "                            )\n",
        "\n",
        "                        # LLM Analiz Sonuçları\n",
        "                        with gr.Accordion(\"Belge Analizi (LLM)\", open=False):\n",
        "                            analysis_text = gr.Textbox(\n",
        "                                label=\"LLM Analizi\",\n",
        "                                interactive=False,\n",
        "                                lines=15,\n",
        "                                placeholder=\"LLM analiz sonuçları burada görünecek.\"\n",
        "                            )\n",
        "                            analysis_info = gr.Textbox(\n",
        "                                label=\"Analiz Bilgisi\",\n",
        "                                interactive=False,\n",
        "                                lines=1\n",
        "                            )\n",
        "\n",
        "            # Sadece sınıflandırma fonksiyonu\n",
        "            def classify_document(file):\n",
        "                if file is None:\n",
        "                    return None, \"Lütfen bir belge yükleyin.\", None, None, [], \"\", \"\", \"\", \"\"\n",
        "\n",
        "                try:\n",
        "                    # Sınıflandırma yap\n",
        "                    result = self.classifier.predict(file.name)\n",
        "                    doc_class_result = result['class']\n",
        "                    confidence_score = result['confidence'] * 100\n",
        "                    summary = f\"Belge '{doc_class_result}' olarak {confidence_score:.2f}% doğrulukla sınıflandırıldı.\"\n",
        "\n",
        "                    # Top 5 sınıf ve olasılıkları\n",
        "                    top_5_classes = []\n",
        "                    for cls, prob in result['sorted_probs'][:5]:\n",
        "                        top_5_classes.append([cls, round(prob * 100, 2)])\n",
        "\n",
        "                    return file.name, summary, doc_class_result, confidence_score, top_5_classes, \"\", \"\", \"\", \"\"\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Sınıflandırma hatası: {str(e)}\"\n",
        "                    return None, error_msg, \"Hata\", 0, [], \"\", \"\", \"\", \"\"\n",
        "\n",
        "            # Sınıflandırma ve OCR fonksiyonu\n",
        "            def process_document(file):\n",
        "                if file is None:\n",
        "                    return None, \"Lütfen bir belge yükleyin.\", None, None, [], \"\", \"\", \"\", \"\"\n",
        "\n",
        "                try:\n",
        "                    # Önce sınıflandırma yap\n",
        "                    classification_result = self.classifier.predict(file.name)\n",
        "                    doc_class_result = classification_result['class']\n",
        "                    confidence_score = classification_result['confidence'] * 100\n",
        "\n",
        "                    # Top 5 sınıf ve olasılıkları\n",
        "                    top_5_classes = []\n",
        "                    for cls, prob in classification_result['sorted_probs'][:5]:\n",
        "                        top_5_classes.append([cls, round(prob * 100, 2)])\n",
        "\n",
        "                    # Sonra metin çıkarma yap\n",
        "                    extraction_start = time.time()\n",
        "                    extraction_result = self.extractor.extract_text(file.name)\n",
        "                    extracted = extraction_result['text']\n",
        "                    extraction_time = time.time() - extraction_start\n",
        "\n",
        "                    # OCR bilgisi\n",
        "                    ocr_info_text = f\"OCR işlemi {extraction_time:.2f} saniyede tamamlandı. Metin uzunluğu: {len(extracted)} karakter.\"\n",
        "\n",
        "                    # Özet oluştur\n",
        "                    if \"error\" in extraction_result['metadata']:\n",
        "                        summary = f\"Belge '{doc_class_result}' olarak sınıflandırıldı. OCR HATASI: {extraction_result['metadata']['error']}\"\n",
        "                    else:\n",
        "                        summary = f\"Belge '{doc_class_result}' olarak {confidence_score:.2f}% doğrulukla sınıflandırıldı. OCR başarılı.\"\n",
        "\n",
        "                    return file.name, summary, doc_class_result, confidence_score, top_5_classes, extracted, ocr_info_text, \"\", \"\"\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"İşleme hatası: {str(e)}\"\n",
        "                    return None, error_msg, \"Hata\", 0, [], \"İşleme sırasında hata oluştu.\", \"\", \"\", \"\"\n",
        "\n",
        "            # Tam analiz fonksiyonu (Sınıflandırma + OCR + LLM Analizi)\n",
        "            def analyze_document(file):\n",
        "                if file is None:\n",
        "                    return None, \"Lütfen bir belge yükleyin.\", None, None, [], \"\", \"\", \"\", \"\"\n",
        "\n",
        "                try:\n",
        "                    # 1. Sınıflandırma\n",
        "                    classification_start = time.time()\n",
        "                    classification_result = self.classifier.predict(file.name)\n",
        "                    doc_class_result = classification_result['class']\n",
        "                    confidence_score = classification_result['confidence'] * 100\n",
        "                    classification_time = time.time() - classification_start\n",
        "\n",
        "                    # Top 5 sınıf ve olasılıkları\n",
        "                    top_5_classes = []\n",
        "                    for cls, prob in classification_result['sorted_probs'][:5]:\n",
        "                        top_5_classes.append([cls, round(prob * 100, 2)])\n",
        "\n",
        "                    # 2. OCR ile metin çıkarma\n",
        "                    extraction_start = time.time()\n",
        "                    extraction_result = self.extractor.extract_text(file.name)\n",
        "                    extracted_text = extraction_result['text']\n",
        "                    extraction_time = time.time() - extraction_start\n",
        "\n",
        "                    # OCR bilgisi\n",
        "                    ocr_info_text = f\"OCR işlemi {extraction_time:.2f} saniyede tamamlandı. Metin uzunluğu: {len(extracted_text)} karakter.\"\n",
        "\n",
        "                    # 3. LLM ile analiz (yeterli metin varsa)\n",
        "                    analysis_text = \"\"\n",
        "                    analysis_info_text = \"\"\n",
        "\n",
        "                    if len(extracted_text) > 50:\n",
        "                        analysis_start = time.time()\n",
        "                        analysis_result = self.analyzer.analyze_document(\n",
        "                            extracted_text,\n",
        "                            doc_class_result\n",
        "                        )\n",
        "                        analysis_text = analysis_result['analysis']\n",
        "                        analysis_time = time.time() - analysis_start\n",
        "                        analysis_info_text = f\"LLM analizi {analysis_time:.2f} saniyede tamamlandı. Analiz uzunluğu: {len(analysis_text)} karakter.\"\n",
        "                    else:\n",
        "                        analysis_text = \"Metin analizi için yeterli içerik bulunamadı (minimum 50 karakter gerekli).\"\n",
        "                        analysis_info_text = \"Analiz yapılamadı: Yetersiz metin.\"\n",
        "\n",
        "                    # Özet oluştur\n",
        "                    summary = f\"Belge '{doc_class_result}' olarak {confidence_score:.2f}% doğrulukla sınıflandırıldı.\"\n",
        "                    if \"error\" in extraction_result['metadata']:\n",
        "                        summary += f\" OCR HATASI: {extraction_result['metadata']['error']}\"\n",
        "                    else:\n",
        "                        summary += f\" OCR ve LLM analizi tamamlandı.\"\n",
        "\n",
        "                    return file.name, summary, doc_class_result, confidence_score, top_5_classes, extracted_text, ocr_info_text, analysis_text, analysis_info_text\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"İşleme hatası: {str(e)}\"\n",
        "                    return None, error_msg, \"Hata\", 0, [], \"İşleme sırasında hata oluştu.\", \"\", \"Analiz yapılamadı.\", \"\"\n",
        "\n",
        "            # Olay bağlama: Dosya seçildiğinde önizleme güncellemesi\n",
        "            file_input.change(\n",
        "                lambda file: file.name if file else None,\n",
        "                inputs=[file_input],\n",
        "                outputs=[image_preview]\n",
        "            )\n",
        "\n",
        "            # Sınıflandırma butonu tıklandığında sadece sınıflandır\n",
        "            classify_btn.click(\n",
        "                classify_document,\n",
        "                inputs=[file_input],\n",
        "                outputs=[image_preview, result_box, doc_class, confidence, top_classes,\n",
        "                         extracted_text, ocr_info, analysis_text, analysis_info]\n",
        "            )\n",
        "\n",
        "            # İşleme butonu tıklandığında sınıflandırma ve OCR yap\n",
        "            process_btn.click(\n",
        "                process_document,\n",
        "                inputs=[file_input],\n",
        "                outputs=[image_preview, result_box, doc_class, confidence, top_classes,\n",
        "                         extracted_text, ocr_info, analysis_text, analysis_info]\n",
        "            )\n",
        "\n",
        "            # Analiz butonu tıklandığında tam analiz yap\n",
        "            analyze_btn.click(\n",
        "                analyze_document,\n",
        "                inputs=[file_input],\n",
        "                outputs=[image_preview, result_box, doc_class, confidence, top_classes,\n",
        "                         extracted_text, ocr_info, analysis_text, analysis_info]\n",
        "            )\n",
        "\n",
        "        return app\n",
        "\n",
        "    def launch(self, share=False):\n",
        "        self.app.launch(share=share, debug=True)  # Debug modunu açtık"
      ],
      "metadata": {
        "id": "Q7jUKTz29UaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECkHmWiyWOfL"
      },
      "outputs": [],
      "source": [
        "# Google Colab için kullanım örneği\n",
        "def start_colab_demo():\n",
        "    # Google Drive'ı bağla\n",
        "    mount_drive()\n",
        "\n",
        "    # Veri seti yolu\n",
        "    data_dir = '/content/drive/MyDrive/RVL_CDIP_small'\n",
        "\n",
        "    # UI başlat - otomatik eğitim/yükleme yapılacak\n",
        "    ui = DocumentProcessingUI(data_dir=data_dir)\n",
        "    ui.launch(share=True)\n",
        "\n",
        "# Ana program\n",
        "if __name__ == \"__main__\":\n",
        "    # Çalıştırma ortamını kontrol et\n",
        "    try:\n",
        "        import google.colab\n",
        "        is_colab = True\n",
        "    except:\n",
        "        is_colab = False\n",
        "\n",
        "    if is_colab:\n",
        "        # Google Colab üzerinde çalışıyorsa\n",
        "        start_colab_demo()\n",
        "    else:\n",
        "        # Normal bir Python ortamında çalışıyorsa\n",
        "        data_dir = \"RVL_CDIP_small\"  # Yerel dizin yolu\n",
        "        ui = DocumentProcessingUI(data_dir=data_dir)\n",
        "        ui.launch()"
      ]
    }
  ]
}